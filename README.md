# Scraper-Profissional-Autom-tico

ğŸ“Œ Resumo

O Scraper Profissional AutomÃ¡tico Ã© um sistema hospedado em servidor que coleta dados profissionais de forma automatizada utilizando Python + Selenium e a API do Google Search para localizar perfis relevantes.
Os dados coletados sÃ£o enviados automaticamente para o Google Sheets (aba "resultados"), permitindo anÃ¡lises imediatas e integraÃ§Ã£o com times de negÃ³cios.

A interface do projeto foi construÃ­da no Lovable, possibilitando disparo, visualizaÃ§Ã£o e acompanhamento do processamento sem necessidade de conhecimento tÃ©cnico.

ğŸ¯ Objetivo

Automatizar a identificaÃ§Ã£o e coleta de perfis profissionais para apoiar prospecÃ§Ã£o, inteligÃªncia comercial e pesquisas de mercado â€” eliminando tarefas manuais e repetitivas.

ğŸ§© Problema

A busca manual por nomes, cargos e empresas em plataformas pÃºblicas Ã©:

demorada

sujeita a erro

difÃ­cil de padronizar

inviÃ¡vel em grande volume

Isso gera perda de produtividade e gargalos para times de negÃ³cios.

ğŸš€ SoluÃ§Ã£o Desenvolvida

Um pipeline de web scraping robusto e escalÃ¡vel composto por:

ğŸ”¹ 1. API Google Search

Localiza automaticamente URLs de perfis pÃºblicos relacionados a nomes, cargos ou termos de pesquisa definidos.

ğŸ”¹ 2. Selenium no Servidor

Abre as URLs, extrai informaÃ§Ãµes estruturadas e padroniza os dados:

Nome

Cargo

Empresa

URL do perfil

ğŸ”¹ 3. Google Sheets API

A base tratada Ã© enviada diretamente para a aba â€œresultadosâ€, sem intervenÃ§Ã£o humana.

ğŸ”¹ 4. Lovable UI

Interface intuitiva com:

BotÃ£o para disparar scraping

Campo para inserir termos de busca

Logs bÃ¡sicos

IndicaÃ§Ã£o de status (processando / finalizado)

ğŸ—ï¸ Arquitetura do Pipeline
Input (Lovable) 
        â†“
API Google Search â†’ URLs encontradas
        â†“
Selenium (Servidor) â†’ Coleta Nome / Cargo / Empresa / Link
        â†“
Tratamento (Pandas)
        â†“
Google Sheets API â†’ Aba "resultados"
        â†“
Interface Lovable exibe status final

ğŸ§  Principais Funcionalidades

Busca inteligente via Google Search API

Scraping automatizado em larga escala

Coleta padronizada de informaÃ§Ãµes profissionais

ExecuÃ§Ã£o via interface Lovable

Logs de execuÃ§Ã£o

Pipeline hospedado em servidor (execuÃ§Ã£o contÃ­nua/estÃ¡vel)

Envio automÃ¡tico para Google Sheets

ğŸ› ï¸ Tecnologias Utilizadas

Python 3

Selenium WebDriver

ChromeDriver Manager

Google Search API

gspread + Google Sheets API

Pandas

Lovable App Interface

Servidor Linux

Git/GitHub

ğŸ“ Estrutura do RepositÃ³rio
/src
   /scraper
       selenium_driver.py
       parser.py
       scraper.py
   /integrations
       google_search_api.py
       sheets_service.py
   /utils
       logs.py
   /server
       run.py

README.md
requirements.txt
interface_lovable.png

from selenium import webdriver
from selenium.webdriver.common.by import By
import pandas as pd
from googleapiclient.discovery import build
import gspread

def search_profiles(query, api_key, cse_id):
    service = build("customsearch", "v1", developerKey=api_key)
    result = service.cse().list(q=query, cx=cse_id).execute()
    return [item['link'] for item in result.get('items', [])]

def extract_profile(driver, url):
    driver.get(url)

    name = driver.find_element(By.CSS_SELECTOR, ".profile-top-card-person-name").text
    cargo = driver.find_element(By.CSS_SELECTOR, ".profile-top-card__summary-item").text
    empresa = driver.find_element(By.CSS_SELECTOR, ".profile-top-card__experience-item").text

    return [name, cargo, empresa, url]
ğŸ“Š Resultados Obtidos

ReduÃ§Ã£o de 80% do tempo de busca manual

Coleta completamente padronizada

Dados centralizados para anÃ¡lises imediatas

Sistema utilizÃ¡vel por qualquer pessoa via interface Lovable

Pipeline replicÃ¡vel para outros tipos de prospecÃ§Ã£o
